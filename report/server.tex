SimpleChubby servers keeps two types of states. First, the
leader server maintains in-memory data structures, which will be lost
after a leader change. Second, the data need to be persistent are duplicated
among leader and follower servers using the consensus protocol.


\subsubsection{In-Memory Data Structures}

SimpleChubby uses a file handler mechanism. The file handler can be very
handy when an application is sensitive to obsolete files. Since the server
rejects any requests with invalid file handlers (i.e. meta-data in the file
handler does not match with server's), the client detects that the node has
been deleted or re-created after receiving a failure of an operation with
the previously opened file handler. The server also reject any fake file
handlers by check the digital signitures in them signed during succeeded
\texttt{fileOpen()} operations.

However, maintaining the information of file handlers on the servers can
be challenging, since queries for file handlers are frequent and the lengths
of the opened file handler lists dynamically change. Therefore, we choose
to keep this information in memory instead of in persistent store, in order
to reduce the overhead of the consensus protocol. In the normal case, a file
handler is create in \texttt{fileOpen()} and destoyed in either
\texttt{fileClose()} or \texttt{fileDelete()}.
All requests from clients except \texttt{fileOpen()} take file handlers as
arguments,
and the server returns failures if the file handlers do not match with
server's.

Another part of in-memory data structures is the queues of outstanding lock
acquire requests for different nodes. Upon any \texttt{acquire()} requests, the
leader server adds the clients' session information into the queues and
blocks the PRC if the locks are held by other clients.
In a \texttt{release()} request, the server pops out a waiting session
(if there are any) on the same lock, modifies the lock owner, and unblocks and
returns the correspongding \texttt{acquire()} request.

We also store clients' request records for events in the leader server's memory.
For each type of event, there are separate lists for different nodes.
The client's session information will be added to the lists during
\texttt{fileOpen()}
request if any event registration flags are asserted. Once a event happens,
the server iterates through the list, and send the event to the client if the
session is still alive. The list is then deleted.



\subsubsection{Persistent Data Store}

The main part of persistent data is the Chubby file system. Each node has
a full path name as the key, a string of content, the meta data defined
in the SimpleChubby API, and a lock owner field. A normal node open returns
the instance number of the node in order to form a file handler, while
a node creation operation triggered by \texttt{fileOpen()} takes a unique ascending
instance number from server and creates a node in the file system. All other
operations on the filesystem (including the lock operations that change lock owners)
take both the name and the instance number of the node as arguments, and will
abort if the node does not exist or the instance number doesn't match the
meta data.

Another persistent data is the value of the next instance number. This global
value is increased by one every time a new node is created, and is passed
to the node creation operation as discussed above. It works as a timestamp,
which helps the server detect and rejects the clients' requests using obsolete
file handlers.
